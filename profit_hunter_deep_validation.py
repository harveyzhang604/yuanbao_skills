#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ’ Profit Hunter DEEP VALIDATION - æ·±åº¦éœ€æ±‚éªŒè¯å¢å¼ºæ¨¡å—
============================================================

æ–°å¢åŠŸèƒ½ï¼š
1. âœ… Redditç—›ç‚¹æŒ–æ˜ï¼ˆæœç´¢ç”¨æˆ·æŠ±æ€¨/ç—›ç‚¹ï¼‰
2. âœ… Googleæœç´¢ç»“æœåˆ†æï¼ˆäººä»¬åœ¨æ‰¾ä»€ä¹ˆè§£å†³æ–¹æ¡ˆï¼‰
3. âœ… éœ€æ±‚çœŸå®æ€§éªŒè¯ï¼ˆæ˜¯å¦æœ‰çœŸå®ç”¨æˆ·éœ€æ±‚ï¼‰
4. âœ… æ·±åº¦Tokenæ¶ˆè€—ï¼ˆå……åˆ†åˆ©ç”¨æ¯åˆ†é’Ÿ50ä¸‡tokené™åˆ¶ï¼‰
5. âœ… æ¯å¤©4æ¬¡è¿è¡Œï¼Œæ¯æ¬¡1å°æ—¶æ·±åº¦åˆ†æ

ä½œè€…ï¼šAI Profit Hunter Team
ç‰ˆæœ¬ï¼š3.0 Deep Validation
æ—¥æœŸï¼š2026-01-30
"""

import os
import sys
import time
import json
import requests
import pandas as pd
from datetime import datetime
from typing import List, Dict, Optional, Tuple
from urllib.parse import quote, urlencode
import warnings
warnings.filterwarnings('ignore')

# ==================== é…ç½®åŒº ====================

DATA_DIR = "data"
REPORTS_DIR = os.path.join(DATA_DIR, "reports")
VALIDATION_DIR = os.path.join(DATA_DIR, "validation")

# æ·±åº¦éªŒè¯é…ç½®
VALIDATION_CONFIG = {
    "REDDIT_SEARCH_LIMIT": 20,       # æ¯ä¸ªå…³é”®è¯æœç´¢Redditçš„å¸–å­æ•°
    "GOOGLE_SERP_LIMIT": 10,         # æ¯ä¸ªå…³é”®è¯æœç´¢Googleçš„ç»“æœæ•°
    "PAIN_KEYWORDS": [                # ç—›ç‚¹ä¿¡å·è¯
        "how to", "can't", "cannot", "problem", "issue", "help",
        "broken", "not working", "struggling", "frustrating", 
        "annoying", "difficult", "hard to", "need", "want",
        "alternative", "better than", "instead of", "wish",
        "there should be", "why is there no"
    ],
    "VALIDATION_THRESHOLD": 3,        # æœ€å°‘éœ€è¦3ä¸ªçœŸå®éœ€æ±‚éªŒè¯
    "MAX_CONCURRENT": 5,              # æœ€å¤§å¹¶å‘éªŒè¯æ•°
}

# ==================== å·¥å…·å‡½æ•° ====================

def ensure_dirs():
    """ç¡®ä¿æ‰€æœ‰å¿…è¦çš„ç›®å½•å­˜åœ¨"""
    os.makedirs(DATA_DIR, exist_ok=True)
    os.makedirs(REPORTS_DIR, exist_ok=True)
    os.makedirs(VALIDATION_DIR, exist_ok=True)

def log_execution(message: str, level: str = "INFO"):
    """æ‰§è¡Œæ—¥å¿—è®°å½•"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{timestamp}] [{level}] {message}")

# ==================== Reddit ç—›ç‚¹æŒ–æ˜ ====================

def search_reddit_pain_points(keyword: str) -> Dict:
    """
    åœ¨Redditæœç´¢å…³é”®è¯ç›¸å…³çš„ç—›ç‚¹è®¨è®º
    
    è¿”å›ï¼š
    {
        "total_mentions": æ•´æ•°,
        "pain_signals": ç—›ç‚¹ä¿¡å·åˆ—è¡¨,
        "real_complaints": çœŸå®æŠ±æ€¨åˆ—è¡¨,
        "validation_score": éœ€æ±‚éªŒè¯åˆ†æ•° (0-100)
    }
    """
    log_execution(f"ğŸ” RedditéªŒè¯: {keyword}")
    
    result = {
        "total_mentions": 0,
        "pain_signals": [],
        "real_complaints": [],
        "validation_score": 0
    }
    
    try:
        # ä½¿ç”¨ Reddit APIï¼ˆä¸éœ€è¦OAuthçš„å…¬å¼€æœç´¢ï¼‰
        search_url = "https://www.reddit.com/search.json"
        params = {
            "q": keyword,
            "limit": VALIDATION_CONFIG["REDDIT_SEARCH_LIMIT"],
            "sort": "relevance",
            "t": "year"  # è¿‡å»ä¸€å¹´
        }
        
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }
        
        response = requests.get(search_url, params=params, headers=headers, timeout=15)
        response.raise_for_status()
        data = response.json()
        
        posts = data.get("data", {}).get("children", [])
        result["total_mentions"] = len(posts)
        
        # åˆ†ææ¯ä¸ªå¸–å­çš„æ ‡é¢˜å’Œå†…å®¹
        pain_count = 0
        for post in posts:
            post_data = post.get("data", {})
            title = post_data.get("title", "").lower()
            selftext = post_data.get("selftext", "").lower()
            combined_text = title + " " + selftext
            
            # æ£€æµ‹ç—›ç‚¹ä¿¡å·
            for pain_keyword in VALIDATION_CONFIG["PAIN_KEYWORDS"]:
                if pain_keyword in combined_text:
                    pain_count += 1
                    result["pain_signals"].append(pain_keyword)
                    
                    # æå–çœŸå®æŠ±æ€¨ï¼ˆæ ‡é¢˜éƒ¨åˆ†ï¼‰
                    if pain_keyword in title and len(title) < 200:
                        result["real_complaints"].append({
                            "text": post_data.get("title", ""),
                            "score": post_data.get("score", 0),
                            "num_comments": post_data.get("num_comments", 0),
                            "url": f"https://reddit.com{post_data.get('permalink', '')}"
                        })
                    break
        
        # è®¡ç®—éªŒè¯åˆ†æ•°
        # å…¬å¼ï¼šç—›ç‚¹ä¿¡å·æ•° * 10 + è¯„è®ºæ•°/10 + ç‚¹èµæ•°/20
        total_comments = sum(p["num_comments"] for p in result["real_complaints"])
        total_score = sum(p["score"] for p in result["real_complaints"])
        
        result["validation_score"] = min(100, 
            len(result["pain_signals"]) * 10 + 
            total_comments / 10 + 
            total_score / 20
        )
        
        log_execution(f"âœ… Reddit: {result['total_mentions']}æ¡è®¨è®º, "
                     f"{len(result['pain_signals'])}ä¸ªç—›ç‚¹ä¿¡å·, "
                     f"éªŒè¯åˆ†æ•°: {result['validation_score']:.1f}")
        
        time.sleep(2)  # ç¤¼è²Œå»¶è¿Ÿ
        
    except Exception as e:
        log_execution(f"âš ï¸ RedditéªŒè¯å¤±è´¥: {str(e)[:100]}", "WARNING")
    
    return result

# ==================== Google SERP éœ€æ±‚åˆ†æ ====================

def analyze_google_serp(keyword: str) -> Dict:
    """
    åˆ†æGoogleæœç´¢ç»“æœï¼Œåˆ¤æ–­éœ€æ±‚ç±»å‹å’Œç«äº‰æƒ…å†µ
    
    è¿”å›ï¼š
    {
        "tool_results_count": å·¥å…·ç±»ç»“æœæ•°é‡,
        "forum_results_count": è®ºå›ç±»ç»“æœæ•°é‡,
        "commercial_intent": å•†ä¸šæ„å›¾å¼ºåº¦ (0-100),
        "has_gap": æ˜¯å¦å­˜åœ¨å¸‚åœºç©ºç™½,
        "top_competitors": å‰3åç«äº‰å¯¹æ‰‹
    }
    """
    log_execution(f"ğŸ” Google SERPéªŒè¯: {keyword}")
    
    result = {
        "tool_results_count": 0,
        "forum_results_count": 0,
        "commercial_intent": 0,
        "has_gap": False,
        "top_competitors": []
    }
    
    try:
        # ä½¿ç”¨ Google Custom Search APIï¼ˆéœ€è¦API Keyï¼‰
        # è¿™é‡Œæä¾›ä¸¤ç§æ–¹æ¡ˆï¼š
        # æ–¹æ¡ˆ1ï¼šç›´æ¥çˆ¬å–Googleæœç´¢ç»“æœï¼ˆç®€å•ä½†å¯èƒ½è¢«é™åˆ¶ï¼‰
        # æ–¹æ¡ˆ2ï¼šä½¿ç”¨ç¬¬ä¸‰æ–¹APIï¼ˆå¦‚ SerpApiã€ValueSerpç­‰ï¼‰
        
        # æ–¹æ¡ˆ1ç¤ºä¾‹ï¼ˆç®€åŒ–ç‰ˆï¼‰
        search_url = "https://www.google.com/search"
        params = {"q": keyword, "num": 10}
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }
        
        response = requests.get(search_url, params=params, headers=headers, timeout=15)
        html = response.text
        
        # ç®€å•åˆ†æï¼ˆå®é™…åº”è¯¥ç”¨BeautifulSoupè§£æï¼‰
        # æ£€æµ‹å·¥å…·ç±»ç½‘ç«™
        tool_domains = ["calculator", "converter", "generator", "tool", "online", "free"]
        for domain in tool_domains:
            result["tool_results_count"] += html.lower().count(domain)
        
        # æ£€æµ‹è®ºå›ç±»ç½‘ç«™
        forum_domains = ["reddit.com", "quora.com", "stackoverflow.com", "forum"]
        for domain in forum_domains:
            result["forum_results_count"] += html.lower().count(domain)
        
        # å•†ä¸šæ„å›¾ï¼ˆå¹¿å‘Šæ•°é‡ï¼‰
        ad_count = html.count('data-text-ad') + html.count('ads-fr')
        result["commercial_intent"] = min(100, ad_count * 10)
        
        # å¸‚åœºç©ºç™½åˆ¤æ–­ï¼šè®ºå›ç»“æœå¤š + å·¥å…·ç»“æœå°‘ = æœ‰éœ€æ±‚ä½†ç¼ºå·¥å…·
        if result["forum_results_count"] >= 3 and result["tool_results_count"] < 5:
            result["has_gap"] = True
        
        log_execution(f"âœ… SERP: {result['tool_results_count']}ä¸ªå·¥å…·, "
                     f"{result['forum_results_count']}ä¸ªè®ºå›, "
                     f"å•†ä¸šæ„å›¾: {result['commercial_intent']}")
        
        time.sleep(3)  # ç¤¼è²Œå»¶è¿Ÿï¼ˆé¿å…è¢«å°ï¼‰
        
    except Exception as e:
        log_execution(f"âš ï¸ SERPéªŒè¯å¤±è´¥: {str(e)[:100]}", "WARNING")
    
    return result

# ==================== ç»¼åˆéœ€æ±‚éªŒè¯ ====================

def deep_validate_keyword(keyword: str) -> Dict:
    """
    å¯¹å•ä¸ªå…³é”®è¯è¿›è¡Œæ·±åº¦éœ€æ±‚éªŒè¯
    
    æ­¥éª¤ï¼š
    1. Redditç—›ç‚¹æŒ–æ˜
    2. Google SERPåˆ†æ
    3. ç»¼åˆåˆ¤æ–­éœ€æ±‚çœŸå®æ€§
    
    è¿”å›ï¼š
    {
        "keyword": å…³é”®è¯,
        "is_real_need": æ˜¯å¦çœŸå®éœ€æ±‚ (True/False),
        "validation_score": ç»¼åˆéªŒè¯åˆ†æ•° (0-100),
        "reddit_data": Redditæ•°æ®,
        "serp_data": SERPæ•°æ®,
        "reasoning": åˆ¤æ–­ç†ç”±
    }
    """
    log_execution(f"\n{'='*60}")
    log_execution(f"ğŸ¯ æ·±åº¦éªŒè¯: {keyword}")
    log_execution(f"{'='*60}")
    
    # Step 1: Redditç—›ç‚¹æŒ–æ˜
    reddit_data = search_reddit_pain_points(keyword)
    
    # Step 2: Google SERPåˆ†æ
    serp_data = analyze_google_serp(keyword)
    
    # Step 3: ç»¼åˆåˆ¤æ–­
    validation_score = 0
    reasoning_points = []
    
    # Redditè´¡çŒ® (50%)
    if reddit_data["validation_score"] > 30:
        validation_score += reddit_data["validation_score"] * 0.5
        reasoning_points.append(f"âœ… Redditæœ‰{reddit_data['total_mentions']}æ¡è®¨è®ºï¼Œ"
                               f"{len(reddit_data['pain_signals'])}ä¸ªç—›ç‚¹ä¿¡å·")
    else:
        reasoning_points.append(f"âš ï¸ Redditè®¨è®ºè¾ƒå°‘({reddit_data['total_mentions']}æ¡)")
    
    # SERPè´¡çŒ® (30%)
    if serp_data["has_gap"]:
        validation_score += 30
        reasoning_points.append("âœ… å‘ç°å¸‚åœºç©ºç™½ï¼šè®ºå›éœ€æ±‚å¤šä½†å·¥å…·å°‘")
    elif serp_data["forum_results_count"] > 0:
        validation_score += 15
        reasoning_points.append(f"âš ï¸ æœ‰è®ºå›è®¨è®º({serp_data['forum_results_count']}ä¸ª)")
    
    # å•†ä¸šæ„å›¾è´¡çŒ® (20%)
    if serp_data["commercial_intent"] > 20:
        validation_score += 20
        reasoning_points.append(f"âœ… å•†ä¸šä»·å€¼é«˜({serp_data['commercial_intent']})")
    
    # æœ€ç»ˆåˆ¤æ–­
    is_real_need = validation_score >= 50
    
    result = {
        "keyword": keyword,
        "is_real_need": is_real_need,
        "validation_score": min(100, validation_score),
        "reddit_data": reddit_data,
        "serp_data": serp_data,
        "reasoning": " | ".join(reasoning_points)
    }
    
    log_execution(f"\nğŸ“Š éªŒè¯ç»“æœ: {'âœ… çœŸå®éœ€æ±‚' if is_real_need else 'âŒ éœ€æ±‚ä¸è¶³'}")
    log_execution(f"ğŸ“ˆ ç»¼åˆå¾—åˆ†: {result['validation_score']:.1f}/100")
    log_execution(f"ğŸ’¡ ç†ç”±: {result['reasoning']}")
    
    return result

# ==================== æ‰¹é‡éªŒè¯ ====================

def batch_validate_keywords(keywords: List[str], max_keywords: int = 20) -> pd.DataFrame:
    """
    æ‰¹é‡éªŒè¯å…³é”®è¯åˆ—è¡¨
    
    å‚æ•°ï¼š
    - keywords: å¾…éªŒè¯çš„å…³é”®è¯åˆ—è¡¨
    - max_keywords: æœ€å¤§éªŒè¯æ•°é‡ï¼ˆæ§åˆ¶è¿è¡Œæ—¶é—´ï¼‰
    
    è¿”å›ï¼š
    DataFrame with validation results
    """
    log_execution(f"\n{'='*60}")
    log_execution(f"ğŸš€ å¼€å§‹æ‰¹é‡éªŒè¯ {min(len(keywords), max_keywords)} ä¸ªå…³é”®è¯")
    log_execution(f"{'='*60}\n")
    
    results = []
    keywords_to_validate = keywords[:max_keywords]
    
    for idx, keyword in enumerate(keywords_to_validate, 1):
        log_execution(f"\n[{idx}/{len(keywords_to_validate)}] éªŒè¯: {keyword}")
        
        validation_result = deep_validate_keyword(keyword)
        results.append(validation_result)
        
        # æ¯éªŒè¯5ä¸ªè¯ï¼Œä¼‘æ¯10ç§’
        if idx % 5 == 0 and idx < len(keywords_to_validate):
            log_execution(f"\nâ¸ï¸ å·²éªŒè¯ {idx} ä¸ªï¼Œä¼‘æ¯10ç§’...")
            time.sleep(10)
    
    # è½¬æ¢ä¸ºDataFrame
    df = pd.DataFrame([
        {
            "keyword": r["keyword"],
            "is_real_need": r["is_real_need"],
            "validation_score": r["validation_score"],
            "reddit_mentions": r["reddit_data"]["total_mentions"],
            "pain_signals": len(r["reddit_data"]["pain_signals"]),
            "real_complaints": len(r["reddit_data"]["real_complaints"]),
            "has_market_gap": r["serp_data"]["has_gap"],
            "commercial_intent": r["serp_data"]["commercial_intent"],
            "reasoning": r["reasoning"]
        }
        for r in results
    ])
    
    # ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_path = os.path.join(VALIDATION_DIR, f"deep_validation_{timestamp}.csv")
    df.to_csv(output_path, index=False, encoding='utf-8-sig')
    
    log_execution(f"\n{'='*60}")
    log_execution(f"âœ… éªŒè¯å®Œæˆï¼ç»“æœä¿å­˜åˆ°: {output_path}")
    log_execution(f"{'='*60}")
    
    # ç»Ÿè®¡
    real_needs = df[df['is_real_need'] == True]
    log_execution(f"\nğŸ“Š éªŒè¯ç»Ÿè®¡:")
    log_execution(f"   æ€»éªŒè¯æ•°: {len(df)}")
    log_execution(f"   âœ… çœŸå®éœ€æ±‚: {len(real_needs)} ({len(real_needs)/len(df)*100:.1f}%)")
    log_execution(f"   âŒ éœ€æ±‚ä¸è¶³: {len(df) - len(real_needs)}")
    log_execution(f"   ğŸ“ˆ å¹³å‡åˆ†: {df['validation_score'].mean():.1f}")
    
    return df

# ==================== ç”Ÿæˆæ·±åº¦éªŒè¯HTMLæŠ¥å‘Š ====================

def generate_deep_validation_report(df: pd.DataFrame, output_path: str = None):
    """ç”Ÿæˆæ·±åº¦éªŒè¯çš„HTMLæŠ¥å‘Š"""
    if output_path is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = os.path.join(REPORTS_DIR, f"deep_validation_report_{timestamp}.html")
    
    # ç­›é€‰å‡ºçœŸå®éœ€æ±‚
    real_needs = df[df['is_real_need'] == True].sort_values('validation_score', ascending=False)
    
    html_content = f"""<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>æ·±åº¦éœ€æ±‚éªŒè¯æŠ¥å‘Š - Profit Hunter</title>
    <style>
        * {{ margin: 0; padding: 0; box-sizing: border-box; }}
        body {{
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            padding: 20px;
            line-height: 1.6;
        }}
        .container {{
            max-width: 1400px;
            margin: 0 auto;
            background: white;
            border-radius: 20px;
            box-shadow: 0 20px 60px rgba(0,0,0,0.3);
            overflow: hidden;
        }}
        .header {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }}
        .header h1 {{
            font-size: 2.5em;
            margin-bottom: 10px;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.2);
        }}
        .content {{
            padding: 40px;
        }}
        .stats {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin-bottom: 30px;
        }}
        .stat-card {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 25px;
            border-radius: 15px;
            text-align: center;
            box-shadow: 0 10px 30px rgba(0,0,0,0.1);
        }}
        .stat-number {{
            font-size: 2.5em;
            font-weight: bold;
            margin-bottom: 5px;
        }}
        .stat-label {{
            font-size: 1em;
            opacity: 0.9;
        }}
        .opportunity {{
            background: linear-gradient(to right, #f8f9fa, #ffffff);
            border-left: 5px solid #667eea;
            padding: 25px;
            margin-bottom: 20px;
            border-radius: 10px;
            box-shadow: 0 5px 15px rgba(0,0,0,0.08);
        }}
        .keyword {{
            background: linear-gradient(135deg, #667eea, #764ba2);
            color: white;
            padding: 8px 15px;
            border-radius: 8px;
            font-weight: bold;
            font-size: 1.1em;
            display: inline-block;
            margin-bottom: 10px;
        }}
        .score-bar {{
            background: #e9ecef;
            height: 30px;
            border-radius: 15px;
            overflow: hidden;
            margin: 10px 0;
        }}
        .score-fill {{
            height: 100%;
            background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
            display: flex;
            align-items: center;
            justify-content: center;
            color: white;
            font-weight: bold;
        }}
        .evidence-box {{
            background: #e8f5e9;
            border-left: 3px solid #4CAF50;
            padding: 15px;
            margin: 15px 0;
            border-radius: 5px;
        }}
        .reasoning {{
            background: #e7f3ff;
            border-left: 3px solid #2196F3;
            padding: 15px;
            margin: 15px 0;
            border-radius: 5px;
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ¯ æ·±åº¦éœ€æ±‚éªŒè¯æŠ¥å‘Š</h1>
            <p>åŸºäºRedditç—›ç‚¹æŒ–æ˜ + Google SERPåˆ†æ</p>
            <p style="opacity: 0.8; margin-top: 10px;">ç”Ÿæˆæ—¶é—´: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}</p>
        </div>
        
        <div class="content">
            <div class="stats">
                <div class="stat-card">
                    <div class="stat-number">{len(df)}</div>
                    <div class="stat-label">éªŒè¯æ€»æ•°</div>
                </div>
                <div class="stat-card">
                    <div class="stat-number">{len(real_needs)}</div>
                    <div class="stat-label">âœ… çœŸå®éœ€æ±‚</div>
                </div>
                <div class="stat-card">
                    <div class="stat-number">{len(real_needs)/len(df)*100:.1f}%</div>
                    <div class="stat-label">éªŒè¯é€šè¿‡ç‡</div>
                </div>
                <div class="stat-card">
                    <div class="stat-number">{df['validation_score'].mean():.1f}</div>
                    <div class="stat-label">å¹³å‡éªŒè¯åˆ†æ•°</div>
                </div>
            </div>
            
            <h2 style="font-size: 2em; margin: 40px 0 20px 0; border-bottom: 3px solid #667eea; padding-bottom: 10px;">
                ğŸ”¥ éªŒè¯é€šè¿‡çš„çœŸå®éœ€æ±‚ (Top {min(20, len(real_needs))})
            </h2>
"""
    
    # æ·»åŠ æ¯ä¸ªéªŒè¯é€šè¿‡çš„å…³é”®è¯
    for idx, (_, row) in enumerate(real_needs.head(20).iterrows(), 1):
        html_content += f"""
            <div class="opportunity">
                <h3>{idx}. {row['keyword']}</h3>
                <div class="keyword">{row['keyword']}</div>
                
                <div class="score-bar">
                    <div class="score-fill" style="width: {row['validation_score']}%;">
                        éªŒè¯åˆ†æ•°: {row['validation_score']:.1f}/100
                    </div>
                </div>
                
                <div class="evidence-box">
                    <strong>ğŸ” éªŒè¯è¯æ®ï¼š</strong><br>
                    â€¢ Redditè®¨è®º: {row['reddit_mentions']}æ¡<br>
                    â€¢ ç—›ç‚¹ä¿¡å·: {row['pain_signals']}ä¸ª<br>
                    â€¢ çœŸå®æŠ±æ€¨: {row['real_complaints']}æ¡<br>
                    â€¢ å¸‚åœºç©ºç™½: {'âœ… æ˜¯' if row['has_market_gap'] else 'âŒ å¦'}<br>
                    â€¢ å•†ä¸šæ„å›¾: {row['commercial_intent']}/100
                </div>
                
                <div class="reasoning">
                    <strong>ğŸ’¡ åˆ¤æ–­ç†ç”±ï¼š</strong><br>
                    {row['reasoning']}
                </div>
            </div>
"""
    
    html_content += """
        </div>
    </div>
</body>
</html>
"""
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(html_content)
    
    log_execution(f"ğŸ“„ HTMLæŠ¥å‘Šå·²ç”Ÿæˆ: {output_path}")
    return output_path

# ==================== ä¸»å‡½æ•° ====================

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Profit Hunter Deep Validation')
    parser.add_argument('--input', type=str, required=True, help='è¾“å…¥CSVæ–‡ä»¶è·¯å¾„ï¼ˆåŒ…å«keywordåˆ—ï¼‰')
    parser.add_argument('--max', type=int, default=20, help='æœ€å¤§éªŒè¯æ•°é‡')
    
    args = parser.parse_args()
    
    ensure_dirs()
    
    # è¯»å–è¾“å…¥æ–‡ä»¶
    if not os.path.exists(args.input):
        log_execution(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input}", "ERROR")
        return
    
    df_input = pd.read_csv(args.input, encoding='utf-8-sig')
    if 'keyword' not in df_input.columns:
        log_execution(f"âŒ è¾“å…¥æ–‡ä»¶å¿…é¡»åŒ…å«'keyword'åˆ—", "ERROR")
        return
    
    keywords = df_input['keyword'].tolist()
    log_execution(f"ğŸ“‚ ä» {args.input} è¯»å–äº† {len(keywords)} ä¸ªå…³é”®è¯")
    
    # æ‰¹é‡éªŒè¯
    df_results = batch_validate_keywords(keywords, max_keywords=args.max)
    
    # ç”ŸæˆHTMLæŠ¥å‘Š
    generate_deep_validation_report(df_results)
    
    log_execution("\nâœ… å…¨éƒ¨å®Œæˆï¼")

if __name__ == "__main__":
    main()
